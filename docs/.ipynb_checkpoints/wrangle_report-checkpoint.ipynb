{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report describes the steps that I took to clean the WeRateDogs data and any challenges that I had in completing those tasks. This was a fairly messy dataset that needed several strategies to make the data able to be analyzed and able to have insights gathered from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step that I did was complete the data gathering process. This involved downloading data manually, programmatically gathering a Twitter archive using `requests`, and using's Twitter's official app API. The bulk of this work was focused on using the Twitter API. Twitter as a social media platform is not like fully open data sources and certain rules and restrictions apply for datasets of given size and type. Though the `tweepy` library allows you to wait on the rate limit using the `wait_on_rate_limit` and `wait_on_rate_limit_notify` options to the `tweepy.API` object instance to follow by Twitter's rate limit and not run into the problem of your Twitter application being locked out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step that I did was identify issues with both data quality and tidyness. All of these problems are outlined in my `wrangle_act.ipynb` notebook. Several main data quality issues were, but not include dog names that made no sense, numerators and denominantors for ratings where the numerator was less than 10, as well as a Tweet source column in the Twitter archive data that was still wrapped in raw HTML and needed to be extracted using a regex. In addition to these items, the tidiness issues that needed addressing were that the data gatehred from the Twitter API needed to be joined to the Twitter archive data, as well as converting stage columns (\"floofer\", \"pupper\" etc.) into a single categorical column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I had assessed what needed to be changed in the datasets in order for them to be cleaned, I drafted a cleaning plan and wrote the code to clean the dataset. The main takeaways were that making a copy of the original datasets helped with correcting mistakes during the cleaning process as well as python's `assert` statements providing a great programmatic check on data quality other than visual confirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all this dataset was a great example on how the data wrangling process works and what needs to be done to actually work with real world datasets. Steps must like these must always be taken to make real world datasets able to analyzed the same way as in program documentation, textbooks, or other situations that involve ideal example data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
